1. Explain HDFS federation and High availability

Answer:

HDFS Federation:
HDFS Federation is included in Hadoop 2.x.
Let HDFS contain two sub-directories in the root directory
/usr/share
• In HDFS federation, there are multiple NameNodes, each storing metadata and block mapping of files and directories contained in particular sub-directories
• The list of sub-directories managed by a NameNode is called a namespace volume
• Blocks for files belonging to a Namespace is called a block pool
For example, here we can have two NameNodes, one for storing the metadata and block mapping for the
namespace volume, /usr and one for /share
• In this way, if one NameNode fails, the namespace volume managed by the other NameNode is still accessible. So that the entire cluster doesn’t become inaccessible
• Now let sub-directories and files inside /usr constitute NameSpace Volume 1
• Sub-directories and files inside /share constitute NameSpace Volume 2
• One DataNode can contain blocks of different namespace volumes
• So, namespace volumes are divided among NameNodes, but not among the DataNodes

High Availability:
• There is a pair of NameNodes in an active-standby configuration
• In the event of the failure of the active NameNode, the standby takes over its duties
without a significant interruption
Architectural Changes:
• The NameNodes must use highly-available shared storage to share the edit log.
Edit logs are read by StandbyNameNode when it takes the responsibility of the
ActiveNameNode
• DataNodes must send blocked reports to both the NameNodes because of the block
mappings
• The secondary NameNode’s role is subsumed by the standby
• StandbyNameNode takes periodic checkpoints of the active NameNode
• Checkpointing is done by Standby NameNode
• Each DataNode reports block stored by it to both NameNodes

2. How HDFS handles failures while writing data
Answer:

1. The pipeline is closed and any packets in the ack queue are added to the front of the data queue
2. The current block on the good DataNodes is given a new identity, which is communicated to the NameNode
3. The failed DataNode is removed from the pipeline, and a new pipeline is constructed from the two good DataNodes
4. The remainder of the block’s data is written to the good DataNodes in the pipeline
5. The NameNode notices that the block is under-replicated, and it arranges for a further replica to be created on another node
6. As long as dfs.namenode.replication.min replicas (which defaults to 1) are written, the write will succeed
7. The block will be asynchronously replicated across the cluster until its target replication factor is reached (dfs.replication, which defaults to 3)
